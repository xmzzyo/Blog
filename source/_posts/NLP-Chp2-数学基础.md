---
title: NLP-Chp2-数学基础
tags: []
thumbnail: ''
mathjax: true
date: 2018-02-02 15:39:04
categories:
	- NLP
description:
---

### 1 概率论基础

> 在自然语言处理中，以句子为处理单位时一般假设句子独立于它前面的其它语句，句子的概率分布近似地符合二项式分布。

### 2 信息论基础

$X离散，p(x)=p(X=x)$

$H(X)=-\sum\limits_{x\in X}P(x)\log_2p(x)$

> 单位是二进制比特位
>
> 自信息
>
> 描述一个随机变量的不确定性。
>
> 一个随机变量的熵越大，它的不确定性越大，正确估计其值的可能性就越小。

**联合熵**

**条件熵**

$H(x, y)=H(x)+H(y|x)$

**相对熵/KL散度**

$D(p||q)=\sum\limits_{x\in X}p(x)\frac{p(x)}{q(x)}$

**交叉熵**

$q(x)用来拟合p(x)，x和q(x)之间的交叉熵为H(X,q)=H(X)+D(p||q)=-\sum\limits_xp(x)\log q(x)$

> 交叉熵的概念用以衡量估计模型与真实概率分布之间
> 的差异

**困惑度**

$2^{H(l,q)}$

**互信息**

$I(x,y)=H(x)-H(x|y)=\log \frac{p(x,y)}{p(x)p(y)}$

<img src="https://cdn.jsdelivr.net/gh/xmzzyo/Blog@master/source/_posts/NLP-Chp2-数学基础/20190114112537.png" style="zoom:50%;" />

> 互信息 I (X; Y) 是在知道了 Y 的值以后 X 的不确定性的减少量，即Y 的值透露了多少关于 X 的信息量。
>
> 互信息值越大，表示两个汉字之间的结合越紧密，越可能成词。反之，断开的可能性越大。

**噪声信道模型**

> 如果我们能够设计一个输入编码 X，其概率分布为 p(X)，使其输入与输出之间的互信息达到最大值，那么，我们的设计就达到了信道的最大传输容量

### 3 应用实例

**词义消歧**

1. 基于上下文分类的消歧方法

   基于贝叶斯分类器

2. 基于最大熵的消歧方法

   > 在只掌握关于未知分布的部分知识的情况下，符合已知知识的概率分布可能有多个，但**使熵值最大的概率分布最真实地反映了事件的分布情况**，因为熵定义了随机变量的不确定性，当熵最大时，随机变量最不确定。也就是说，**在已知部分知识的前提下，关于未知分布最合理的推断应该是符合已知知识最不确定或最大随机的推断。**

   > 下文条件由如下三类信息表示:
   >
   > 1. 特征的类型：词形、词性、词形＋词性，3种情况；
   > 2. 上下文窗口大小：当前词的左右2个词，1种情况；
   > 3. 是否考虑位置：是或否，2种情况。
   >
   > 上述3种情况组合，可得到如下 n 种特征模板：
   > n＝3×1×2＝6