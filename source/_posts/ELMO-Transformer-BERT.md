---
title: ELMO & Transformer & BERT
tags: []
thumbnail: ''
mathjax: true
date: 2019-01-13 10:16:50
categories:
	- NLP
description:
---

## Deep contextualized word representations

### Abstract

(1)complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy).

### Introduction

ELMo representations are deep, in the sense that they are a function of all of the internal layers of the biLM.

## Attention is All you need

[Transformer PyTorch 实现](http://nlp.seas.harvard.edu/2018/04/03/attention.html)

[Transformer 动图讲解](https://jalammar.github.io/illustrated-transformer/)

