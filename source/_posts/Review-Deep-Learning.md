---
title: Review Deep Learning
tags: []
thumbnail: ''
mathjax: true
date: 2018-06-11 21:20:04
categories:
	- DL
description:
---

## DL Basics

![](https://cdn.jsdelivr.net/gh/xmzzyo/Blog@master/source/_posts/Review-Deep-Learning/58736544.jpg)

![](https://cdn.jsdelivr.net/gh/xmzzyo/Blog@master/source/_posts/Review-Deep-Learning/40136780.jpg)

### Linear Algebra

#### Scalars

#### Vectors

$\mathbb{R^n}$

#### Matrices

2-D Array

$\mathbb{R^{m\times n}}$

#### Tensors

#### Matrix Transpose

<img src="https://cdn.jsdelivr.net/gh/xmzzyo/Blog@master/source/_posts/Review-Deep-Learning/18790749.jpg" style="zoom:50%;" />

$(AB)^T=B^TA^T$

#### Matrix (Dot) Product

#### Identity Matrix

$I_nx=x$

#### Systems of Equations

åªæœ‰ä¸€ä¸ªè§£ï¼šå¯é€†

#### Matrix Inversion

å¯é€†å¿…è¦æ¡ä»¶ï¼šæ–¹é˜µï¼Œæ»¡ç§©

#### Norms

$||x||_1=\sum_i|x_i|$

$||x||_ \infty =\max\limits_i|x_i|$

#### Special Matrices and Vectors

æ­£äº¤é˜µï¼š$A^{-1}=A^T$

#### Eigendecomposition

æ¯ä¸€ä¸ªå®å¯¹ç§°çŸ©é˜µéƒ½æœ‰å®ï¼Œæ­£äº¤ç‰¹å¾åˆ†è§£ï¼š

$A=Q \land Q^T$

#### SVD

ä¸ç”¨æ˜¯æ–¹é˜µ

#### Moore-Penrose Pseudoinverse

ï¼Ÿï¼Ÿï¼Ÿ

#### Trace

### Probability and Information Theory

#### Computing Marginal Probability with Sum Rule

$P(X=x)=\sum_yP(X=x|Y=y)$

$p(x)=\int p(x,y)dy$

#### Bernoulli Distribution

<img src="https://cdn.jsdelivr.net/gh/xmzzyo/Blog@master/source/_posts/Review-Deep-Learning/30844542.jpg" style="zoom:50%;" />

#### Gaussian Distribution

<img src="https://cdn.jsdelivr.net/gh/xmzzyo/Blog@master/source/_posts/Review-Deep-Learning/15520315.jpg" style="zoom:50%;" />

<img src="https://cdn.jsdelivr.net/gh/xmzzyo/Blog@master/source/_posts/Review-Deep-Learning/60061288.jpg" style="zoom:50%;" />

## CNN

### ç‰¹ç‚¹

1. Sparse interactions

    ä¸æ˜¯å…¨é“¾æ¥ï¼Œç¨€ç–é“¾æ¥

2. Parameter sharing

    æ•´å¼ å›¾ç‰‡å…±äº«ä¸€ä¸ªkernelå‚æ•°çŸ©é˜µ

3. Equivariant representations

    $f(g(x))=g(f(x))$

    Images: If we move an object in the image, its representation will move the same amount in the output

    Convolution is not equivariant to other operations such as change in scale or rotation

4. Ability to work with inputs of variable size

### Poolingä¼˜ç‚¹

1. Pooling helps the representation become slightly invariant to small translations of the input(we care more about whether a certain feature is present rather than exactly where it is)
2. Since pooling is used for downsampling, it can be used to handle inputs of varying sizes

### Convolution

***è¾“å‡ºå¤§å°:***

$\frac{N-K}{S}+1$

$Nï¼ŒåŸå›¾å¤§å°(é•¿æˆ–è€…å®½)ï¼ŒKï¼Œkernelï¼ŒSï¼Œæ­¥é•¿$

***Zero Padding***

$\frac{K-1}{2}\ paddingå¯ä»¥ä¿ç•™åŸæ¥çš„size$

<img src="https://cdn.jsdelivr.net/gh/xmzzyo/Blog@master/source/_posts/Review-Deep-Learning/48854382.jpg" style="zoom:50%;" />

## RNN

### LSTM

Challenge of Long-Term Dependenciesï¼šæ¢¯åº¦æ¶ˆå¤±æˆ–çˆ†ç‚¸

<img src="https://cdn.jsdelivr.net/gh/xmzzyo/Blog@master/source/_posts/Review-Deep-Learning/19687725.jpg" style="zoom:50%;" />

LSTMå¯ä»¥è§£å†³æ¢¯åº¦æ¶ˆå¤±ï¼ˆå¼€å¿˜è®°é—¨ï¼‰ï¼Œä¸èƒ½è§£å†³æ¢¯åº¦çˆ†ç‚¸

The influence never disappears unless forget gate is closed

No Gradient vanishing (If forget gate is opened.)

Instead of computing new state as a matrix product with the old state, it rather computes the difference between them.  Expressivity is the same, but gradients are better behaved.

ç»“æ„ï¼š

<img src="https://cdn.jsdelivr.net/gh/xmzzyo/Blog@master/source/_posts/Review-Deep-Learning/27510155.jpg" style="zoom:50%;" />

### GRUç»“æ„

<img src="https://cdn.jsdelivr.net/gh/xmzzyo/Blog@master/source/_posts/Review-Deep-Learning/10544389.jpg" style="zoom:50%;" />

Exploding is controlled with gradient clipping. Vanishing is controlled with additive interactions (LSTM)

## æ­£åˆ™åŒ–å’Œä¼˜åŒ–

Regularization is any modification made to the learning algorithm with an intention to **lower the generalization error but not the training error**.

### ç»å…¸æ­£åˆ™åŒ–ç­–ç•¥

1. Parameter Norm Penalties

> L2 norm penalty can be interpreted as a **MAP Bayesian** 
> **Inference with a Gaussian prior on the weights**.

>L1 norm penalty can be interpreted as a 
>**MAP Bayesian Inference with a Isotropic Laplace Distribution**
>**prior on the weights.**

2. Dataset Augmentation

3. Noise Robustness

> Noise added to weights 
>
> Noise Injection on Outputs. An example is label smoothing.

4. Early Stopping
5. Parameter Sharing
6. Parameter Tying
7. Multitask Learning
8. Bagging
9. Ensemble Models
10. Dropout

> Dropout can intuitively be explained as forcing the model to learn with **missing input and hidden units**.

> Each time we load an example into a minibatch, we randomly sample a different binary mask to apply to all of the input and hidden units in the network.

11. Adversarial Training
> training on adversarially perturbed examples from the training set.

### ä¼˜åŒ–æ–¹æ³•

1. Gradient Descent

   - Batch Gradient Descent

   > Need to compute gradients over the entire training for one update

   - Stochastic Gradient Descent

2. Minibatching

   > Use larger mini-batches

3. Learning Rate Schedule

   > the learning rate is decayed linearly

4. Momentum

   > The Momentum method is a method to accelerate learning using SGD

   æ¢¯åº¦ï¼š

   ![](https://cdn.jsdelivr.net/gh/xmzzyo/Blog@master/source/_posts/Review-Deep-Learning/36643891.jpg)

   ![](https://cdn.jsdelivr.net/gh/xmzzyo/Blog@master/source/_posts/Review-Deep-Learning/25566880.jpg)

5. Nesterov Momentum

   <img src="https://cdn.jsdelivr.net/gh/xmzzyo/Blog@master/source/_posts/Review-Deep-Learning/34867821.jpg" style="zoom:50%;" />

6. AdaGrad

   ![](https://cdn.jsdelivr.net/gh/xmzzyo/Blog@master/source/_posts/Review-Deep-Learning/60918544.jpg)

7. RMSProp

   ![](https://cdn.jsdelivr.net/gh/xmzzyo/Blog@master/source/_posts/Review-Deep-Learning/58611138.jpg)

8. Adam

   ![](https://cdn.jsdelivr.net/gh/xmzzyo/Blog@master/source/_posts/Review-Deep-Learning/59169403.jpg)

***ä»¥ä¸Šæ–¹æ³•æ¯”è¾ƒ***

![](https://cdn.jsdelivr.net/gh/xmzzyo/Blog@master/source/_posts/Review-Deep-Learning/44066790.jpg)

9. Batch Normalization

> Let H be a design matrix having activations in any layer for m examples in the mini-batch

<img src="https://cdn.jsdelivr.net/gh/xmzzyo/Blog@master/source/_posts/Review-Deep-Learning/80725967.jpg" style="zoom:50%;" />

![](https://cdn.jsdelivr.net/gh/xmzzyo/Blog@master/source/_posts/Review-Deep-Learning/93754219.jpg)

***ä¼˜ç‚¹***
- Improves gradient flow through the network.
- Allows higher learning rates.
- Reduces the strong dependence on initialization.
- Acts as a form of regularization in a funny way, and slightly reduces the need for dropout.

10. Initialization Strategies

> **Initialization should break symmetry (quiz!)**

![](https://cdn.jsdelivr.net/gh/xmzzyo/Blog@master/source/_posts/Review-Deep-Learning/57101764.jpg)

## Reinforcement Learning

<img src="https://cdn.jsdelivr.net/gh/xmzzyo/Blog@master/source/_posts/Review-Deep-Learning/88055126.jpg" style="zoom:50%;" />

### Model-free learning

1. **Policy-based Approach** Learning an Actor

- Step1: Neural Network as Actor
> Input of neural network: the observation of machine represented as a vector or a matrix

> Output neural network : each action corresponds to a 
> neuron in output layer

- Step 2: goodness of function
> Given an actor $ğœ‹_ğœƒ ğ‘ $ with network parameter $ğœƒ$

![](https://cdn.jsdelivr.net/gh/xmzzyo/Blog@master/source/_posts/Review-Deep-Learning/51784666.jpg)


- Step 3: pick the best function
  Policy Gradient

![](https://cdn.jsdelivr.net/gh/xmzzyo/Blog@master/source/_posts/Review-Deep-Learning/892430.jpg)

![](https://cdn.jsdelivr.net/gh/xmzzyo/Blog@master/source/_posts/Review-Deep-Learning/6836615.jpg)

![](https://cdn.jsdelivr.net/gh/xmzzyo/Blog@master/source/_posts/Review-Deep-Learning/27246838.jpg)

2. **Value-based Approach** Learning a Critic

> A critic does not determine the action.
>
> Given an actor Ï€, it evaluates the how good the actor is

**Critic**

![](https://cdn.jsdelivr.net/gh/xmzzyo/Blog@master/source/_posts/Review-Deep-Learning/72949772.jpg)

> Monte-Carlo based approach 
>
> The critic watches ğœ‹ playing the game

**MC VS. TD**

![](https://cdn.jsdelivr.net/gh/xmzzyo/Blog@master/source/_posts/Review-Deep-Learning/52393914.jpg)

![](https://cdn.jsdelivr.net/gh/xmzzyo/Blog@master/source/_posts/Review-Deep-Learning/93543592.jpg)

**Q-Learning**

![](https://cdn.jsdelivr.net/gh/xmzzyo/Blog@master/source/_posts/Review-Deep-Learning/9105331.jpg)

3. **Deep Reinforcement Learning** Actor-Critic

<img src="https://cdn.jsdelivr.net/gh/xmzzyo/Blog@master/source/_posts/Review-Deep-Learning/57243000.jpg" style="zoom:50%;" />

### Model-based learning

<img src="https://cdn.jsdelivr.net/gh/xmzzyo/Blog@master/source/_posts/Review-Deep-Learning/16408984.jpg" style="zoom:50%;" />

**Advantages of Model-Based RL**
<img src="https://cdn.jsdelivr.net/gh/xmzzyo/Blog@master/source/_posts/Review-Deep-Learning/19484532.jpg" style="zoom: 33%;" />

<img src="https://cdn.jsdelivr.net/gh/xmzzyo/Blog@master/source/_posts/Review-Deep-Learning/43402611.jpg" style="zoom: 33%;" />


