---
title: Note-深度强化学习
tags: []
thumbnail: ''
mathjax: true
toc: true
date: 2020-04-19 19:34:53
categories:
description:
---

# Deep Reinforcement Learning

> Note from:
>
> - https://github.com/nndl/nndl.github.io

## 概念

### 定义

> 强化学习（Reinforcement Learning，RL），是指一类从**（与环境）交互中不断学习**的问题以及解决这类问题的方法. 强化学习问题可以描述为一个智能体从与环境的交互中不断学习以完成特定目标（比如取得最大奖励
> 值）. 和深度学习类似，强化学习中的关键问题也是贡献度分配问题，每一个动作并不能直接得到监督信息，需要通过整个模型的最终监督信息（奖励）得到，并且有一定的**延时**性.

**例子：**

- 多臂老虎机（Multi-Armed Bandit Problem）
- 悬崖行走

**两个对象：**

1. 智能体（Agent）可以感知外界**环境的状态**（State）和**反馈的奖励**（Reward），并进行**学习和决策**.
2. 环境（Environment）是智能体外部的所有事物，并受智能体**动作**的影响而**改变其状态**，并**反馈**给智能体相应的**奖励**.

**五个基本元素：**

1. 状态𝑠 是对环境的表示，可以是离散的或连续的，其状态空间为$\mathcal{S}$.
2. 动作𝑎 是对智能体的行为，可以是离散的或连续的，其动作空间为$\mathcal{A}$.
3. 策略𝜋(𝑎|𝑠) 是**智能体**根据环境状态𝑠 来决定下一步动作𝑎 的函数.
   - 确定性策略：𝜋 ∶ 𝒮 → 𝒜
   - 随机性策略：选择动作的概率分布𝜋(𝑎|𝑠) ≜ 𝑝(𝑎|𝑠)
   - 一般使用随机性策略. 随机性策略可以更好地探索环境，并具有多样性
4. 状态转移概率𝑝(𝑠′|𝑠, 𝑎) 是在**智能体**根据当前状态𝑠 做出一个动作𝑎 之后，环境在下一个时刻转变为状态𝑠′ 的概率.
5. 即时奖励𝑟(𝑠, 𝑎, 𝑠′) 是一个标量函数，即智能体根据当前状态𝑠 做出动作𝑎 之后，环境会反馈给智能体一个奖励，**这个奖励也经常和下一个时刻的状态𝑠′有关**.

### 马尔科夫决策过程

**交互序列：**

$$𝑠_0, 𝑎_0, 𝑠_1, 𝑟_1, 𝑎_1, ⋯ , 𝑠_{𝑡−1}, 𝑟_{𝑡−1}, 𝑎_{𝑡−1}, 𝑠_𝑡, 𝑟_𝑡, ⋯ $$

其中$𝑟_𝑡 = 𝑟(𝑠_{𝑡−1}, 𝑎_{𝑡−1}, 𝑠_𝑡) $是第𝑡 时刻的即时奖励

<img src="../asset/Note-%E6%B7%B1%E5%BA%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/image-20200420110155864.png" alt="image-20200420110155864" style="zoom:50%;" />

**马尔可夫决策过程**（Markov Decision Process，**MDP**）在**马尔可夫**过程中加入一个额外的变量：动作𝑎，即下一个时刻的**状态$𝑠_{𝑡+1}$ 和当前时刻的状态$𝑠_𝑡$ 以及动作$𝑎_𝑡$ 相关(一阶马尔可夫)**，

$𝑝(𝑠_{𝑡+1}|𝑠_𝑡, 𝑎_𝑡, ⋯ , 𝑠_0, 𝑎_0) = 𝑝(𝑠_{𝑡+1}|𝑠_𝑡, 𝑎_𝑡)$

其中$𝑝(𝑠_{𝑡+1}|𝑠_𝑡, 𝑎_𝑡) $为状态转移概率.

给定策略𝜋(𝑎|𝑠)，马尔可夫决策过程的一个**轨迹（Trajectory）**：

$𝜏 = 𝑠_0, 𝑎_0, 𝑠_1, 𝑟_1, 𝑎_1, ⋯ , 𝑠_{𝑇−1}, 𝑎_{𝑇−1}, 𝑠_𝑇 , 𝑟_𝑇$

其概率为：

$𝑝(𝜏) = 𝑝(𝑠_0)\prod_{t=0}^{T-1}𝜋(𝑎_𝑡|𝑠_𝑡)𝑝(𝑠_{𝑡+1}|𝑠_𝑡, 𝑎_𝑡).$

<img src="../asset/Note-%E6%B7%B1%E5%BA%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/image-20200420112406554.png" alt="image-20200420112406554" style="zoom:50%;" />

### 目标函数

轨迹𝜏 一个回合（Episode）或试验（Trial）所收到的累积奖励：

$𝐺(𝜏) =\sum_{t=0}^{T-1}𝑟(𝑠_𝑡, 𝑎_𝑡, 𝑠_{𝑡+1})$

引入一个**折扣率**来**降低远期回报的权重**. 折扣回报（Discounted Return）:

$G(𝜏) =\sum_{t=0}^{T-1}𝛾^𝑡𝑟_{𝑡+1}$

**𝛾 ∈ [0, 1] 是折扣率**. 当𝛾 接近于0 时，智能体**更在意短期回报**；而当𝛾 接近于1 时，**长期回报变得更重要**.

**最大化期望回报**（Expected Return）:

$𝒥(𝜃) = 𝔼𝜏∼𝑝_𝜃(𝜏)[𝐺(𝜏)] = 𝔼𝜏∼𝑝_𝜃(𝜏)[\sum_{t=0}^{T-1}𝛾^𝑡𝑟_{𝑡+1}]$

### 值函数（状态值函数和状态-动作值函数）

#### 状态值函数：

<img src="../asset/Note-%E6%B7%B1%E5%BA%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/image-20200420115023853.png" alt="image-20200420115023853" style="zoom:67%;" />

𝑉𝜋(𝑠) 称为状态值函数（State Value Function），表示**从状态𝑠 开始（到结束）**，执行策略𝜋 得到的期望总回报

$𝑉𝜋(𝑠) = 𝔼𝜏∼𝑝(𝜏)[\sum_{t=0}^{T-1}𝛾^𝑡𝑟_{𝑡+1}|𝜏_{𝑠_0} = 𝑠]$

**贝尔曼方程（Bellman Equation）**（动态规划），表示当前状态的值函数可以通过下个状态的值函数来计算.

**递推公式：**

<img src="../asset/Note-%E6%B7%B1%E5%BA%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/image-20200420115742034.png" alt="image-20200420115742034" style="zoom:67%;" />

如果给定策略𝜋(𝑎|𝑠)，状态转移概率𝑝(𝑠′|𝑠, 𝑎) 和奖励𝑟(𝑠, 𝑎, 𝑠′)，我们就可以通过迭代的方式来计算$𝑉^𝜋(𝑠)$. 由于存在折扣率，迭代一定步数后，每个状态的值函数就会固定不变.

#### 状态-动作值函数（Q 函数）

$𝑄^𝜋(𝑠, 𝑎) = 𝔼𝑠′∼𝑝(𝑠′|𝑠,𝑎) [𝑟(𝑠, 𝑎, 𝑠′) + 𝛾𝑉^𝜋(𝑠′)]$

状态值函数$𝑉^𝜋(𝑠)$ 是Q 函数$𝑄^𝜋(𝑠, 𝑎)$ **关于动作𝑎 的期望**，

$𝑉^𝜋(𝑠) = 𝔼𝑎∼𝜋(𝑎|𝑠)[𝑄^𝜋(𝑠, 𝑎)]$

Q函数的贝尔曼方程：

<img src="../asset/Note-%E6%B7%B1%E5%BA%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/image-20200420120648270.png" alt="image-20200420120648270" style="zoom:67%;" />

值函数的作用→优化：

> 假设在状态𝑠，有一个动作$𝑎^∗ $使得$𝑄^𝜋(𝑠, 𝑎^∗) > 𝑉^𝜋(𝑠)$，说明执行动作$𝑎^∗ $的回报比当前的策略𝜋(𝑎|𝑠) 要高，我们就可以调整参数使得策略中执行动作$𝑎^∗ $的概率$𝑝(𝑎^∗|𝑠)$ 增加.

### 深度强化学习

神经网络建模策略$𝜋(𝑎|𝑠)$ 和值函数$𝑉^𝜋(𝑠), 𝑄^𝜋(𝑠, 𝑎)$

## 值函数方法（Value/Critic）

> 先随机**初始化一个策略**，计算该策略的值函数，并根据值函数来设置新的策略，然后一直反复迭代直到收敛.
>
> 基于值函数的策略学习方法中最关键的是如何计算策略𝜋 的值函数，有动态规划或蒙特卡罗两种计算方式.

### 动态规划

通过贝尔曼方程来迭代计算其值函数. 这种**模型已知**的强化学习算法也称为**基于模型的强化学习（Model-Based Reinforcement Learning）**算法，这里的**模型**就是指**马尔可夫决策过程**.

**限制：**

1. **要求模型已知**，即要给出马尔可夫决策过程的状态转移概率𝑝(𝑠′|𝑠, 𝑎) 和奖励函数𝑟(𝑠, 𝑎, 𝑠′)，这个要求很难满足. 
2. 效率问题，即当状态数量较多时，算法效率比较低.

### 蒙特卡罗方法(MC)

> 基于采样的学习算法也称为模型无关的强化学习（Model-Free Reinforcement Learning）算法.

#### 方法

Q 函数$𝑄^𝜋(𝑠, 𝑎)$ 为初始状态为𝑠，并执行动作𝑎 后所能得到的期望总回报，可以写为:

$𝑄^𝜋(𝑠, 𝑎) = 𝔼𝜏∼𝑝(𝜏)[𝐺(𝜏_{𝑠_0}=𝑠,𝑎_0=𝑎)]$

其中$𝜏_{𝑠_0}=𝑠,𝑎_0=𝑎$ 表示轨迹𝜏 的**起始状态和动作为𝑠, 𝑎.**

如果模型未知，Q 函数可以通过采样来进行计算，这就是蒙特卡罗方法.

假设我们进行𝑁 次试验，得到𝑁 个轨迹𝜏(1), 𝜏(2), ⋯ , 𝜏(𝑁)，其总回报分别为$𝐺(𝜏^{(1)}), 𝐺(^{𝜏(2)}), ⋯ , 𝐺(𝜏^{(𝑁)})$. Q 函数可以近似为

$𝑄^𝜋(𝑠, 𝑎) ≈ \hat{𝑄}^𝜋(𝑠, 𝑎) = \frac{1}{N}\sum_{n=1}^N𝐺(𝜏^{(𝑛)}_{𝑠_0=𝑠,𝑎_0=𝑎})$

在近似估计出Q 函数之后，就可以进行策略改进. 然后在新的策略下重新通过采样来估计Q 函数，并不断重复，直至收敛.

#### Exploitation and Exploration

𝜖-贪心法（𝜖-greedy Method）：

<img src="../asset/Note-%E6%B7%B1%E5%BA%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/image-20200420152525225.png" alt="image-20200420152525225" style="zoom:50%;" />

每次选择动作𝜋(𝑠) 的概率为$1 − 𝜖 + \frac{𝜖}{|𝒜|}$，其他动作的概率为 $\frac{𝜖}{|𝒜|}$.

#### 同策略（on-policy）和异策略（off-policy）

**同策略：**

 在蒙特卡罗方法中，如果**采样策略**是$𝜋^𝜖(𝑠)$，不断**改进策略也是**$𝜋^𝜖(𝑠)$ **而不是目标策略**𝜋(𝑠). 这种**采样与改进策略相同**（即都是$𝜋^𝜖(𝑠)$）的强化学习方法叫做同策略（On-Policy）方法.

**异策略:**

 如果**采样策略**是$𝜋^𝜖(𝑠)$，而**优化目标**是策略𝜋，可以**通过重要性采样**，**引入重要性权重**来实现对目标策略𝜋 的优化这种**采样与改进分别使用不同策略**的强化学习方法叫做异策略（Off-Policy）方法.

### 时序差分学习方法(TD)

蒙特卡罗方法一般**需要拿到完整的轨迹**，才能对策略进行评估并更新模型，因此效率也比较低.

















